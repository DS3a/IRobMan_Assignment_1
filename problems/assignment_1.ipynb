{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "The following assignment is going to cover lectures 2, 3 & 4.\n",
    "\n",
    "## Broad Topics are as follows:\n",
    "- Rigid Body\n",
    "- Differential Kinematics\n",
    "- Geometric Perception\n",
    "\n",
    "# Maximum Marks possible: [$60 = 20 + 20 + 20$]\n",
    "\n",
    "If individual marks for a subpart is not specified assume equal marks for all the subparts within that section.\n",
    "\n",
    "# How to Answer ?:\n",
    "\n",
    "For theoretical questions make a markdown cell below each question/question-subpart and write the answer there.\n",
    "For coding questions write the answer at the designated position indicated by the question.\n",
    "\n",
    "# Submission:\n",
    "You are have to submit the file `assignment_1.ipynb` with your answers into moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Rigid Body [Full Marks: $20 = 10 + 10$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: [Marks: $10 = 4 + 6$]\n",
    "\n",
    "Remember the Rodrigues formula for converting an axis of rotation into a rotation matrix: \n",
    "\n",
    "$$R = I + (1 - cos(\\theta)) K^2 + sin(\\theta) K \\quad \\text{- eqn(1)}$$\n",
    "\n",
    "Where $K = \\begin{bmatrix} 0 & -k_z & k_y \\\\ k_z & 0 & -k_x \\\\ -k_y & k_x & 0 \\end{bmatrix}$\n",
    "\n",
    "We briefly touched upon the fact that this formula gives us a speak into the world of Lie Groups and Lie Algebra's i.e. $R \\in SO(3)$ and the $\\theta K \\in so(3)$\n",
    "\n",
    "The following question has x parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Question 1 Part 1: [Marks: $4$]\n",
    "\n",
    "Write down the characteristic polynomial of the matrix $K$ in it's most simplified form. (Hint. Don't forget what K is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 Part 2: [Marks: $6$]\n",
    "\n",
    "Now using the formula given in the eqn(1) find a single function $f$ such that $R = f(\\theta K)$ (Hint. Cayley Hamilton Theorem, Taylor Series)\n",
    "\n",
    "Note that $f$ should be in it's simplest form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: [Marks: $10=3+4+3$]\n",
    "\n",
    "There exists another fun way of expressing 3D rotations and is one widely used in real life computer vision and graphics applications.\n",
    "\n",
    "They are called quaternions and it was created as an extension of the complex numer system.\n",
    "\n",
    "It is an expression of the form $a + b \\mathbf{i} + c \\mathbf{j} + d \\mathbf{k}$\n",
    "\n",
    "They follow the following properties:\n",
    "\n",
    "- A set of quaternions form a $4$-dimensional vector space.\n",
    "- Each component of a quaternion follows a multiplication rule similar to vectors with some similarities to complex numbers as well.\n",
    "\n",
    "### Multiplication Table:\n",
    "\n",
    "$$\\mathbf{i}^2 = \\mathbf{j}^2 = \\mathbf{k}^2 = -1$$\n",
    "$$\\mathbf{i} \\mathbf{j} = -\\mathbf{j} \\mathbf{i} =  \\mathbf{k}$$\n",
    "$$\\mathbf{j} \\mathbf{k} = -\\mathbf{k} \\mathbf{j} =  \\mathbf{i}$$\n",
    "$$\\mathbf{k} \\mathbf{i} = -\\mathbf{i} \\mathbf{k} =  \\mathbf{j}$$\n",
    "$$\\mathbf{i} \\mathbf{j} \\mathbf{k} = -1$$\n",
    "\n",
    "We will learn more about quaternion with the coding assignment below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A very basic quaternion class\n",
    "class Quaternion:\n",
    "    def __init__(self, w=0, x=0, y=0, z=0):\n",
    "        self.w = w  # real/scalar part\n",
    "        self.x = x  # i component\n",
    "        self.y = y  # j component\n",
    "        self.z = z  # k component\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Quaternion(self.w + other.w,\n",
    "                          self.x + other.x,\n",
    "                          self.y + other.y,\n",
    "                          self.z + other.z)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Quaternion(self.w - other.w,\n",
    "                          self.x - other.x,\n",
    "                          self.y - other.y,\n",
    "                          self.z - other.z)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Quaternion multiplication\n",
    "        # ---COMPLETE-THE-REST OF THE CODE (Question 2 Part 1)---\n",
    "\n",
    "        return Quaternion(w, x, y, z)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.w:.2f} + {self.x:.2f}i + {self.y:.2f}j + {self.z:.2f}k\"\n",
    "\n",
    "\n",
    "def quaternion_to_rotation_matrix(q: Quaternion) -> np.ndarray:\n",
    "    # ---COMPLETE-THE-REST OF THE CODE (Question 2 Part 2)---\n",
    "\n",
    "    return R\n",
    "\n",
    "\n",
    "def exp_map_so3(axis: np.ndarray, angle: float) -> np.ndarray:\n",
    "    # ---COMPLETE-THE-REST OF THE CODE (Question 2 Part 3)---\n",
    "    if np.abs(angle) < 1e-10:\n",
    "        return np.eye(3)\n",
    "\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 Part 1 [Marks: $3$]\n",
    "\n",
    "Based on the above definition of multiplication can you complete the multiplication function between two quaternions.\n",
    "Write your answer in the `def __mul__(self, other):` function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 Part 2 [Marks: $4$]\n",
    "\n",
    "In our lectures we saw how to convert between different representations of rotation. The relationship between a quaternion and axis angle is as follows:\n",
    "\n",
    "For an axis angle representation $k = ([k_x, k_y, k_z], \\theta)$ and a quaternion $q = (w, x, y, z)$ where $k$ is a unit vector. \n",
    "\n",
    "$$ w = cos(\\theta/2), \\ x = k_x sin(\\theta / 2), \\ y = k_y sin(\\theta/2), \\ z = k_z sin(\\theta/2)$$\n",
    "\n",
    "Knowing what you know about the Rotation Matrix and axis angle representation, can you create a formula for directly convetring a quaternion $q$ into a rotation matrix.\n",
    "\n",
    "First derive the result and then code up the solution in: `def quaternion_to_rotation_matrix(q: Quaternion)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Question 2 Part 3 [Marks: $3$]\n",
    "\n",
    "Implement the exponential map that we derived in Question 1 in `def exp_map_so3(axis: np.ndarray, angle: float)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But why did we make all these functions:\n",
    "\n",
    "These transformations not only help us with making intresting conversions but also helps us **interpolate between rotations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_quaternion(q: Quaternion) -> Quaternion:\n",
    "    \"\"\"Normalize a quaternion\"\"\"\n",
    "    norm = np.sqrt(q.w**2 + q.x**2 + q.y**2 + q.z**2)\n",
    "    return Quaternion(q.w/norm, q.x/norm, q.y/norm, q.z/norm)\n",
    "\n",
    "\n",
    "def quaternion_interpolation(q1: Quaternion, q2: Quaternion, t: float) -> Quaternion:\n",
    "    \"\"\"Spherical Linear Interpolation between quaternions\"\"\"\n",
    "    # Normalize quaternions\n",
    "    q1 = normalize_quaternion(q1)\n",
    "    q2 = normalize_quaternion(q2)\n",
    "\n",
    "    # Compute the cosine of the angle between quaternions\n",
    "    cos_theta = q1.w*q2.w + q1.x*q2.x + q1.y*q2.y + q1.z*q2.z\n",
    "\n",
    "    # Ensure we take the shortest path\n",
    "    if cos_theta < 0:\n",
    "        q2 = Quaternion(-q2.w, -q2.x, -q2.y, -q2.z)\n",
    "        cos_theta = -cos_theta\n",
    "\n",
    "    # If quaternions are too close, perform linear interpolation\n",
    "    if cos_theta > 0.9995:\n",
    "        return normalize_quaternion(Quaternion(\n",
    "            w=(1-t)*q1.w + t*q2.w,\n",
    "            x=(1-t)*q1.x + t*q2.x,\n",
    "            y=(1-t)*q1.y + t*q2.y,\n",
    "            z=(1-t)*q1.z + t*q2.z\n",
    "        ))\n",
    "\n",
    "    # Interpolation on the surface of a hypersphere\n",
    "    theta = np.arccos(cos_theta)\n",
    "    sin_theta = np.sin(theta)\n",
    "\n",
    "    s1 = np.sin((1-t)*theta) / sin_theta\n",
    "    s2 = np.sin(t*theta) / sin_theta\n",
    "\n",
    "    return normalize_quaternion(Quaternion(\n",
    "        w=s1*q1.w + s2*q2.w,\n",
    "        x=s1*q1.x + s2*q2.x,\n",
    "        y=s1*q1.y + s2*q2.y,\n",
    "        z=s1*q1.z + s2*q2.z\n",
    "    ))\n",
    "\n",
    "\n",
    "def rotation_interpolation(R1: np.ndarray, R2: np.ndarray, t: float) -> np.ndarray:\n",
    "    \"\"\"Linear interpolation between rotation matrices using axis-angle representation.\"\"\"\n",
    "\n",
    "    # Ensure valid rotation matrices\n",
    "    if not (np.allclose(np.dot(R1, R1.T), np.eye(3)) and np.allclose(np.dot(R2, R2.T), np.eye(3))):\n",
    "        raise ValueError(\"Input matrices must be valid rotation matrices\")\n",
    "\n",
    "    # Here we implement the logarithmic map with some additional conditions for stability\n",
    "    # Convert matrices to axis-angle representation\n",
    "    theta1 = np.arccos(np.clip((np.trace(R1) - 1)/2, -1.0, 1.0))\n",
    "    theta2 = np.arccos(np.clip((np.trace(R2) - 1)/2, -1.0, 1.0))\n",
    "\n",
    "    axis1 = np.array([\n",
    "            R1[2,1] - R1[1,2],\n",
    "            R1[0,2] - R1[2,0],\n",
    "            R1[1,0] - R1[0,1]\n",
    "        ]) / (2 * np.sin(theta1))\n",
    "        \n",
    "    axis2 = np.array([\n",
    "        R2[2,1] - R2[1,2],\n",
    "        R2[0,2] - R2[2,0],\n",
    "        R2[1,0] - R2[0,1]\n",
    "    ]) / (2 * np.sin(theta2))\n",
    "    \n",
    "    # Handle special cases\n",
    "    if theta1 < 1e-10:  # R1 is identity\n",
    "        if theta2 < 1e-10:  # Both are identity\n",
    "            return np.eye(3)\n",
    "        axis = axis2\n",
    "        theta = t * theta2\n",
    "    elif theta2 < 1e-10:  # R2 is identity\n",
    "        axis = axis1\n",
    "        theta = (1-t) * theta1\n",
    "    else:\n",
    "        # Normalize axes\n",
    "        axis1 = axis1 / np.linalg.norm(axis1)\n",
    "        axis2 = axis2 / np.linalg.norm(axis2)\n",
    "        \n",
    "        # Check if axes are opposite\n",
    "        if np.dot(axis1, axis2) < 0:\n",
    "            axis2 = -axis2\n",
    "            theta2 = -theta2\n",
    "            \n",
    "        # Linear interpolation of axis and angle\n",
    "        theta = (1-t)*theta1 + t*theta2\n",
    "        axis = (1-t)*axis1 + t*axis2\n",
    "        axis = axis / np.linalg.norm(axis)  # Ensure normalized axis\n",
    "    # Convert axis-angle representation to rotation matrix\n",
    "    return exp_map_so3(axis, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_rotation_axis(ax, axis, scale=1.0, label_offset=0.1):\n",
    "    \"\"\"Plot the rotation axis as a dashed line with label\"\"\"\n",
    "    # Plot axis line\n",
    "    axis_normalized = axis / np.linalg.norm(axis)\n",
    "    axis_line = np.vstack([-scale * axis_normalized, scale * axis_normalized])\n",
    "    ax.plot3D(axis_line[:, 0], axis_line[:, 1], axis_line[:, 2], \n",
    "              'k--', linewidth=2, label='Rotation Axis')\n",
    "    \n",
    "    # Add label with arrow\n",
    "    label_pos = (scale + label_offset) * axis_normalized\n",
    "    ax.text(label_pos[0], label_pos[1], label_pos[2], \n",
    "            'Rotation\\nAxis', \n",
    "            fontsize=10, \n",
    "            horizontalalignment='center')\n",
    "\n",
    "\n",
    "def create_coordinate_frame(scale=0.3):\n",
    "    \"\"\"Create a coordinate frame with RGB arrows\"\"\"\n",
    "    vertices = []\n",
    "    connections = []\n",
    "    colors = []\n",
    "    \n",
    "    def create_arrow_vertices(direction, color):\n",
    "        base = np.zeros(3)\n",
    "        tip = direction * scale\n",
    "        \n",
    "        # Arrow head parameters\n",
    "        head_length = scale * 0.2\n",
    "        head_width = scale * 0.1\n",
    "        \n",
    "        # Calculate arrow head vertices\n",
    "        if direction[0] == 1:  # X-axis\n",
    "            head1 = tip - [head_length, head_width, 0]\n",
    "            head2 = tip - [head_length, -head_width, 0]\n",
    "            head3 = tip - [head_length, 0, head_width]\n",
    "            head4 = tip - [head_length, 0, -head_width]\n",
    "            label_pos = tip + [0.05, 0, 0]\n",
    "            label = \"X\"\n",
    "        elif direction[1] == 1:  # Y-axis\n",
    "            head1 = tip - [head_width, head_length, 0]\n",
    "            head2 = tip - [-head_width, head_length, 0]\n",
    "            head3 = tip - [0, head_length, head_width]\n",
    "            head4 = tip - [0, head_length, -head_width]\n",
    "            label_pos = tip + [0, 0.05, 0]\n",
    "            label = \"Y\"\n",
    "        else:  # Z-axis\n",
    "            head1 = tip - [head_width, 0, head_length]\n",
    "            head2 = tip - [-head_width, 0, head_length]\n",
    "            head3 = tip - [0, head_width, head_length]\n",
    "            head4 = tip - [0, -head_width, head_length]\n",
    "            label_pos = tip + [0, 0, 0.05]\n",
    "            label = \"Z\"\n",
    "        \n",
    "        start_idx = len(vertices)\n",
    "        vertices.extend([base, tip, head1, head2, head3, head4, label_pos])\n",
    "        \n",
    "        # Add connections\n",
    "        connections.extend([\n",
    "            (start_idx, start_idx + 1),  # Main arrow line\n",
    "            (start_idx + 1, start_idx + 2),  # Head lines\n",
    "            (start_idx + 1, start_idx + 3),\n",
    "            (start_idx + 1, start_idx + 4),\n",
    "            (start_idx + 1, start_idx + 5)\n",
    "        ])\n",
    "        \n",
    "        colors.extend([color] * 5)  # One color for each connection\n",
    "        return label, label_pos\n",
    "    \n",
    "    # Create arrows and store label information\n",
    "    labels = []\n",
    "    label_positions = []\n",
    "    \n",
    "    x_label, x_pos = create_arrow_vertices(np.array([1, 0, 0]), 'r')  # X-axis (red)\n",
    "    y_label, y_pos = create_arrow_vertices(np.array([0, 1, 0]), 'g')  # Y-axis (green)\n",
    "    z_label, z_pos = create_arrow_vertices(np.array([0, 0, 1]), 'b')  # Z-axis (blue)\n",
    "    \n",
    "    labels.extend([x_label, y_label, z_label])\n",
    "    label_positions.extend([x_pos, y_pos, z_pos])\n",
    "    \n",
    "    return np.array(vertices), connections, colors, labels, np.array(label_positions)\n",
    "\n",
    "\n",
    "def plot_coordinate_frame(ax, R, vertices, connections, colors, labels, label_positions):\n",
    "    \"\"\"Plot the coordinate frame after applying rotation R\"\"\"\n",
    "    # Apply rotation to vertices\n",
    "    rotated_vertices = (R @ vertices.T).T\n",
    "    rotated_labels_pos = (R @ label_positions.T).T\n",
    "    \n",
    "    # Plot each connection with its color\n",
    "    for (start, end), color in zip(connections, colors):\n",
    "        ax.plot3D(rotated_vertices[[start, end], 0],\n",
    "                 rotated_vertices[[start, end], 1],\n",
    "                 rotated_vertices[[start, end], 2],\n",
    "                 color=color, linewidth=2)\n",
    "    \n",
    "    # Add labels for axes\n",
    "    for label, pos in zip(labels, rotated_labels_pos):\n",
    "        ax.text(pos[0], pos[1], pos[2], label, fontsize=10)\n",
    "\n",
    "\n",
    "def plot_rotation_axis(ax, axis, scale=1.0, label_offset=0.1):\n",
    "    \"\"\"Plot the rotation axis as a dashed line with label\"\"\"\n",
    "    # Plot axis line\n",
    "    axis_normalized = axis / np.linalg.norm(axis)\n",
    "    axis_line = np.vstack([-scale * axis_normalized, scale * axis_normalized])\n",
    "    ax.plot3D(axis_line[:, 0], axis_line[:, 1], axis_line[:, 2], \n",
    "              'k--', linewidth=2, label='Rotation Axis')\n",
    "    \n",
    "    # Add label with arrow\n",
    "    label_pos = (scale + label_offset) * axis_normalized\n",
    "    ax.text(label_pos[0], label_pos[1], label_pos[2], \n",
    "            'Rotation\\nAxis', \n",
    "            fontsize=10, \n",
    "            horizontalalignment='center')\n",
    "\n",
    "\n",
    "def visualize_rotation_interpolation(rotation_axis=np.array([1, 1, 1]),\n",
    "                                     rotation_angle=np.pi/1.5):\n",
    "    # Create initial and final rotations\n",
    "    q1 = Quaternion(1, 0, 0, 0)  # Identity quaternion\n",
    "    # Axis angle to quaternion transformation\n",
    "    angle = rotation_angle\n",
    "    axis = rotation_axis\n",
    "    axis = axis / np.linalg.norm(axis)\n",
    "    q2 = Quaternion(np.cos(angle/2),\n",
    "                    axis[0]*np.sin(angle/2),\n",
    "                    axis[1]*np.sin(angle/2),\n",
    "                    axis[2]*np.sin(angle/2))\n",
    "    q1 = normalize_quaternion(q1)\n",
    "    q2 = normalize_quaternion(q2)\n",
    "\n",
    "    # Create initial and final rotation matrices\n",
    "    R1 = quaternion_to_rotation_matrix(q1)\n",
    "    R2 = quaternion_to_rotation_matrix(q2)\n",
    "\n",
    "    # Create coordinate frame\n",
    "    vertices, connections, colors, labels, label_positions = create_coordinate_frame()\n",
    "\n",
    "    # Create interpolation points\n",
    "    t_values = np.linspace(0, 1, 50)\n",
    "\n",
    "    # Create figure once\n",
    "    plt.close('all')  # Clean up any existing plots\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    # Add text annotation for rotation angle\n",
    "    angle_deg = np.degrees(angle)\n",
    "    angle_text = f'Rotation Angle: {angle_deg:.1f}°'\n",
    "\n",
    "    # Animation loop\n",
    "    for i, t in enumerate(t_values):\n",
    "        # Clear the current figure but keep the window\n",
    "        plt.clf()\n",
    "        \n",
    "        # Create new subplots\n",
    "        ax1 = fig.add_subplot(121, projection='3d')\n",
    "        ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "        # Quaternion interpolation\n",
    "        q_interp = quaternion_interpolation(q1, q2, t)\n",
    "        R_q = quaternion_to_rotation_matrix(q_interp)\n",
    "\n",
    "        # Matrix interpolation\n",
    "        R_m = rotation_interpolation(R1, R2, t)\n",
    "\n",
    "        # Plot coordinate frames\n",
    "        plot_coordinate_frame(ax1, R_q, vertices, connections, colors, labels, label_positions)\n",
    "        plot_coordinate_frame(ax2, R_m, vertices, connections, colors, labels, label_positions)\n",
    "\n",
    "        # Set plot properties\n",
    "        for ax, title in [(ax1, 'Quaternion Interpolation'),\n",
    "                         (ax2, 'Matrix Interpolation')]:\n",
    "            # Plot rotation axis\n",
    "            plot_rotation_axis(ax, axis)\n",
    "            \n",
    "            # Add rotation angle text\n",
    "            ax.text2D(0.05, 0.95, angle_text,\n",
    "                     transform=ax.transAxes,\n",
    "                     fontsize=10,\n",
    "                     bbox=dict(facecolor='white', alpha=0.8))\n",
    "            \n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "            ax.set_zlabel('Z')\n",
    "            ax.set_xlim([-1, 1])\n",
    "            ax.set_ylim([-1, 1])\n",
    "            ax.set_zlim([-1, 1])\n",
    "            ax.set_box_aspect([1,1,1])\n",
    "            \n",
    "            # Add legend\n",
    "            ax.legend(loc='upper right')\n",
    "            \n",
    "            # Set view angle\n",
    "            ax.view_init(elev=30, azim=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Only display during animation, show() at the end\n",
    "        if i < len(t_values) - 1:\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "            time.sleep(0.05)\n",
    "        else:\n",
    "            # For the final frame, use plt.show()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with code:\n",
    "\n",
    "Change the `rotation_axis` and `rotation_angle` to view the 3D arrow coordinate rotating around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_rotation_interpolation(rotation_axis=np.array([1, 1, 1]), rotation_angle = np.pi/1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Differential Kinematics [Full Marks: $20 = 10 + 10$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question 1: Inverse Kinematics [Marks: $10$]\n",
    "\n",
    "An animation of a typical box grasping IK can be shown below:\n",
    "\n",
    "<img src=\"../data/ik.gif\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the library **robotic** to achieve a simple ik, more can be found at https://github.com/MarcToussaint/robotic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robotic as ry\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "C = ry.Config()\n",
    "C.addFile(ry.raiPath('scenarios/pandaSingle.g'))\n",
    "C.view()\n",
    "\n",
    "C.addFrame('box') \\\n",
    "    .setPosition([-.25,.1,1.]) \\\n",
    "    .setShape(ry.ST.ssBox, size=[.06,.06,.06,.005]) \\\n",
    "    .setColor([1,.5,0]) \\\n",
    "    .setContact(1)\n",
    "C.view()\n",
    "\n",
    "qHome = C.getJointState()\n",
    "\n",
    "komo = ry.KOMO(C, 1,1,0, True)\n",
    "komo.addObjective([], ry.FS.jointState, [], ry.OT.sos, [1e-1], qHome)\n",
    "komo.addObjective([], ry.FS.accumulatedCollisions, [], ry.OT.eq)\n",
    "komo.addObjective([], ry.FS.jointLimits, [], ry.OT.ineq)\n",
    "komo.addObjective([], ry.FS.positionDiff, ['l_gripper', 'box'], ry.OT.eq, [1e1])\n",
    "komo.addObjective([], ry.FS.scalarProductXX, ['l_gripper', 'box'], ry.OT.eq, [1e1], [0])\n",
    "komo.addObjective([], ry.FS.scalarProductXZ, ['l_gripper', 'box'], ry.OT.eq, [1e1], [0])\n",
    "komo.addObjective([], ry.FS.distance, ['l_palm', 'box'], ry.OT.ineq, [1e1])\n",
    "\n",
    "ret = ry.NLP_Solver(komo.nlp(), verbose=0 ) .solve()\n",
    "print(ret)\n",
    "if ret.feasible:\n",
    "    print('-- Always check feasibility flag of NLP solver return')\n",
    "else:\n",
    "    print('-- THIS IS INFEASIBLE!')\n",
    "\n",
    "q = komo.getPath()\n",
    "C.setJointState(q[0])\n",
    "C.view(False, \"IK solution\") # if you can not run this function, just comment this line\n",
    "gripper_pos = C.getFrame('l_gripper').getPosition()\n",
    "box = C.getFrame('box')\n",
    "p0 = box.getPosition() # memory the start box position\n",
    "print(\"pos diff\", gripper_pos-p0)\n",
    "\n",
    "for t in range(10):\n",
    "    box.setPosition(p0 + .2 * np.random.randn(3)) # randomize box position\n",
    "    komo.updateRootObjects(C) # only works for root objects (the 'box' is one)\n",
    "    ret = ry.NLP_Solver(komo.nlp(), verbose=0 ) .solve()\n",
    "    print(ret)\n",
    "    q = komo.getPath()\n",
    "    C.setJointState(q[0])\n",
    "    gripper_pos = C.getFrame('l_gripper').getPosition()\n",
    "    box_pos = C.getFrame('box').getPosition()\n",
    "    print(\"pos diff \", gripper_pos-box_pos)\n",
    "    C.view(False, 'IK solution - ' + ('*** INFEASIBLE ***' if not ret.feasible else 'feasible'))\n",
    "    time.sleep(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script, the key to achieve IK is to set clever constraints. Here, we focus on one constraint specifically, `scalarProduct`. The two lines of `scalarProductXX` and `scalarProductXZ` state that the gripper x-axis (which is the axis connecting the fingers) should be orthogonal to the object x- and z-axes.\n",
    "\n",
    "Now, we also need to evaluate the other two cases: `the gripper x-axis is orthogonal to the object x- and y-axes`, and `the gripper x-axis is orthogonal to the object y- and z-axes` for **each randomized position**. For each case, we should keep track of the performance `100.*(ret.eq+ret.ineq) + ret.sos` and select the best IK pose with the leaset score. (**HINT: you may need a list to store the komo instances with three different gripper constraints**).\n",
    "\n",
    "Please complete the code in the following block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########TODO############\n",
    "del komo\n",
    "komo = []\n",
    "\n",
    "\n",
    "\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question 2: Bimanual Manipulation [Marks: $10$]\n",
    "\n",
    "Given that we have two robots and they are required to carry an object at the same time. \n",
    "\n",
    "<img src=\"../data/mobile_minis.gif\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep a **fixed relative position** of the two end effectors **in the world frame**. Given that we have the kinematic tree of the multi-robot system, the function to return the positions of the end effectors $r0\\_gripper\\_pos(q), r1\\_gripper\\_pos(q)$, and the size of the object $[w, l ,h]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 Part 1: \n",
    "How do we formulate the constraint function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 Part 2: \n",
    "\n",
    "Given that the DoF for each robot is 8, so the **joint configuration** $q \\in R^{16}$. \n",
    "What is the size of the Jacobian matrix of the constraint function wrt $q$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 Part 3:\n",
    "\n",
    "Given that we have the Jacobian matrix that maps the **joint configuration** $q$ to the position of one end effector $r0\\_gripper\\_pos(q)$.\n",
    "If there is a zero submatrix within the Jacobian matrix, where is the zero submatrix?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Question 2 Part 4:\n",
    "To decompose the motion planning problem of the multi-robot systems into smaller problems. We randomly sample a configuration $q$.\n",
    "\n",
    "<img src=\"../data/rrt_random.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "But since we randomly sample the configuration, there is no guarantee the end effectors will satisfy the constraints. \n",
    "\n",
    "Can you write the pseudocode to generate a configuration $q*$ to satisfy the constraints?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Geometric Perception [Full Marks: $20 = 10 + 10$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question 1: Methods for Calculating Gradients: Manual vs. Autograd [Marks: $10$]\n",
    "### Using Pytorch autograd feature to minimize errors\n",
    "\n",
    "* torch.autograd is PyTorch's automatic differentiation engine that powers neural network training, but we can use it for other optimization purposes. We adapt it here to solve our perspective transformation fitting problem at hand.\n",
    "\n",
    "> https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "\n",
    "> https://pytorch.org/tutorials/beginner/pytorch_with_examples.html?highlight=autograd\n",
    "\n",
    "* Generally speaking, torch.autograd is an engine for computing **vector-Jacobian product**! Study the materials at the above links for more indepth understanding, if you so desire.\n",
    "\n",
    "* Here is a link to Prof. Grosse's lecture on autodiff/autograd, which provides more technical information about how autograd works.\n",
    "\n",
    "> https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf\n",
    "\n",
    "\n",
    "Some important observations:\n",
    "\n",
    "* \"Conceptually, autograd keeps a record of data (tensors) & all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects. In this DAG, leaves are the input tensors, and roots are the output tensors. By tracing this graph from roots to leaves, you can **automatically compute the gradients using the chain rule.**\"\n",
    "\n",
    "\n",
    "* \"**DAGs are dynamic in PyTorch** An important thing to note is that the graph is recreated from scratch; after each .backward() call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.\"\n",
    "\n",
    "* \"torch.autograd tracks operations on all tensors which have their requires_grad flag set to True. For tensors that don't require gradients, setting this attribute to False excludes it from the gradient computation DAG.\"\n",
    "\n",
    "* \"The output tensor of an operation will require gradients even if only a single input tensor has requires_grad=True.\"\n",
    "\n",
    "* \"It is useful to \"freeze\" part of your model if you know in advance that you won't need the gradients of those parameters (this offers some performance benefits by reducing autograd computations).\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd is especially useful when you have chained functions.\n",
    "\n",
    "\\begin{equation}\n",
    "z = f(x, a) \\\\\n",
    "y = g(z, b)\n",
    "\\end{equation}\n",
    "\n",
    "where $a$ and $b$ are parameters. We are interested in the derivatives of $y$ with respect to $a$ and $b$: $\\frac{\\partial y}{\\partial a}$ and $\\frac {\\partial y}{\\partial b}$\n",
    "\n",
    "We you were do this the old fashion way, you will have to derive expressions for these derivatives and then code them.\n",
    "\n",
    "$\\frac {\\partial y}{\\partial b}  = \\frac {\\partial g(z, b)}{\\partial b}$\n",
    "\n",
    "$\\frac {\\partial y}{\\partial a}  = \\frac {\\partial y}{\\partial z} \\frac {\\partial z}{\\partial a} = \\frac {\\partial g(z, b)}{\\partial z} \\frac {\\partial f(x, a)}{\\partial a}$\n",
    "\n",
    "Note the use of chain rule for the second one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case when $f(x, a) = ax^2$ and $g(z, b) = b/z$\n",
    "\n",
    "$\\frac {\\partial y}{\\partial b}  = 1/z$\n",
    "\n",
    "$\\frac {\\partial y}{\\partial a}  = \\frac {\\partial y}{\\partial z} \\frac {\\partial z}{\\partial a} = \\frac {\\partial g(z, b)}{\\partial z} \\frac {\\partial f(x, a)}{\\partial a} = -(b/z^2)x^2$\n",
    "\n",
    "Let us say we have to compute these derivatives at $x=2, a = 3, b = 4$. \n",
    "\n",
    "**Let's implement both methods: manually and using autograd.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f (x, a) :\n",
    "    ############ TODO ##############\n",
    "\n",
    "    ################################\n",
    "\n",
    "def g (z, b):\n",
    "    ############ TODO ##############\n",
    "\n",
    "    ################################\n",
    "\n",
    "def z_deriv_a (x, a) :\n",
    "    ############ TODO ##############\n",
    "\n",
    "    ################################\n",
    "\n",
    "def y_deriv_z (z, b) :\n",
    "    ############ TODO ##############\n",
    "\n",
    "    ################################\n",
    "    \n",
    "\n",
    "def y_deriv_b (z, b) :\n",
    "    ############ TODO ##############\n",
    "\n",
    "    ################################\n",
    "   \n",
    "\n",
    "x = 2\n",
    "a = 3\n",
    "b = 4\n",
    "z = f(x, a)\n",
    "print('y deriv a: %3.2f'%(y_deriv_z (z, b) * z_deriv_a (x, a)), '\\ny deriv b:%3.2f'%(y_deriv_b (z, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autograd**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([3.], requires_grad=True)\n",
    "b = torch.tensor([4.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=False)\n",
    "\n",
    "def f (x, a) :\n",
    "    return (a*(x**2))\n",
    "\n",
    "def g (z, b):\n",
    "    return (b/z)\n",
    "\n",
    "################ TODO ###########\n",
    "\n",
    "#################################\n",
    "# check if collected gradients are correct\n",
    "print('y_a: %3.2f \\ny_b: %3.2f'%(a.grad.item(), b.grad.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wishing you access to powerful tools in the future.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: RANSAC with Gaussian-Newton Method [Marks: $10$]\n",
    "### Random Sample Consensus (RANSAC)\n",
    "*Random* Sample Consensus (RANSAC) is a method to fit a model to the observed data points in the presence of outliers.\n",
    "\n",
    "RANSAC was proposed in 1981 by Fischler and Bolles (M.A. Fischler and R.C. Bolles: _[Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography](http://www.cs.ait.ac.th/~mdailey/cvreadings/Fischler-RANSAC.pdf)_. Communications of the ACM, 24(6):381–395, 1981).\n",
    "### Non-linear Least Squares methods\n",
    "\n",
    "> 1. We start with an initial rough estimate $\\mathbf{p_0}$, which could be $\\mathbf{p_0} = \\mathbf{0}$ but are best-fit affine parameters in practice.\n",
    "> 2. Then we iteratively add increments, $\\Delta \\mathbf{p}$, to the parameters to minimize the residual at each step.\n",
    "\n",
    "The following are some ways to solve non-linear optimization problems:\n",
    "\n",
    "> 1. [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent), a method from 1847, is used for backpropagation by deep learning methods.\n",
    "> 2. [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) (a.k.a. the Newton–Raphson method) uses the second derivative (Hessian) and the gradient (Jacobian).\n",
    "> 3. [Gauss-Newton method](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm), which we can only use to minimize a **sum of squared function values**, has the advantage that second derivatives are not needed.\n",
    "> 4. [Levenberg–Marquardt algorithm](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) is used to solve non-linear least-squares problems and is the most commonly used stable approach that dynamically trades off between the Gauss-Newton approach and gradient descent.\n",
    "\n",
    "* We experiment with gradient descent and the Newton approach here.\n",
    "\n",
    "* The Newton method uses the second derivative (or Hessian) to compute its step sizes\n",
    "\n",
    "* Gradient descent is very slow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as scimage\n",
    "from skimage.transform import warp\n",
    "import torch.nn as nn\n",
    "from skimage import io\n",
    "from io import BytesIO\n",
    "from skimage.transform import warp\n",
    "\n",
    "# Installing the version of OpenCV that has SIFT\n",
    "#!pip install opencv-contrib-python==4.4.0.44\n",
    "import cv2\n",
    "print (cv2 .__version__)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "\n",
    "def detect_and_match_keypoints (image_1, image_2) :\n",
    "    # input are just two images\n",
    "    # returns SIFT key points and descriptors for each image along with sorted match list (plus pairs of point matching coordinates)\n",
    "    # SIFT with default parameters\n",
    "    sift = cv2.SIFT_create(nOctaveLayers = 3, contrastThreshold = 0.04, edgeThreshold = 10, sigma = 1.6)\n",
    "\n",
    "    keypoints_1, descriptors_1 = sift.detectAndCompute(image_1, None)\n",
    "    keypoints_2, descriptors_2 = sift.detectAndCompute(image_2, None)\n",
    "\n",
    "    # FEATURE MATCHING\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "\n",
    "    matches = bf.match(descriptors_1, descriptors_2)\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    X_1 = []\n",
    "    X_2 = []\n",
    "    for i in range(len(matches)) :\n",
    "        X_1.append([keypoints_1[matches[i].queryIdx].pt[0], keypoints_1[matches[i].queryIdx].pt[1]])\n",
    "        X_2.append([keypoints_2[matches[i].trainIdx].pt[0], keypoints_2[matches[i].trainIdx].pt[1]])\n",
    "    X_1 = np.array(X_1)\n",
    "    X_2 = np.array(X_2)\n",
    "\n",
    "    return(X_1, X_2, keypoints_1, keypoints_2, matches)\n",
    "\n",
    "def qr_solve (H, b) :\n",
    "    # H is n by n matrix, b is a n by 1 vector\n",
    "    # returns a n by 1 vector as a solution\n",
    "\n",
    "    Q, R = np.linalg.qr(H, 'reduced')\n",
    "    b_dash = Q.transpose(1,0) @ b\n",
    "    Np = b_dash.shape[0]\n",
    "    del_p = np.zeros((Np, 1))\n",
    "    for i in range(Np-1, -1, -1): # work from the last row of R\n",
    "        sum_r_p = [0]\n",
    "        for j in range(i+1, Np, 1) :\n",
    "            sum_r_p += R[i, j]*del_p[j]\n",
    "        if (R[i,i] != 0.0):\n",
    "            del_p[i] = (b_dash[i]- sum_r_p)/R[i,i]\n",
    "\n",
    "    return(del_p)\n",
    "\n",
    "def fit_affine (X, X_dash, select_flag) :\n",
    "    # input: two 2D points sets, 3 by N arrays of homogeneous representation of the points\n",
    "    # select_flag: 1D array of 0 an 1 indicating which points to use in the estimation.\n",
    "    # output: residual of fit and the best fitting affine transformation\n",
    "\n",
    "    # Compute the matrix H from the point coordinate moments\n",
    "    M = X @ np.diag(select_flag) @ X.transpose(1,0)\n",
    "    H1 = np.column_stack((M, np.zeros((3,3))))\n",
    "    H2 = np.column_stack((np.zeros((3,3)), M))\n",
    "    H = np.row_stack((H1, H2))\n",
    "\n",
    "    # vector b\n",
    "    b_dash = X @ np.diag(select_flag) @(X_dash - X).transpose(1,0)\n",
    "    b = np.row_stack((b_dash[:,0][:,None], b_dash[:,1][:,None]))\n",
    "\n",
    "    p = qr_solve (H, b)\n",
    "\n",
    "    p = p.squeeze()\n",
    "    # the parameter vector is [a_00, a_01, t_x, a_10, a_11, t_y]\n",
    "    # rearrange it back into homogeneous matrix representation\n",
    "    # do not forget to add in the identity matrix. We used the (1+a_ii) parameterization\n",
    "    T_affine = np.row_stack((p.reshape(2, 3), [0, 0, 0])) + np.eye(3)\n",
    "\n",
    "    X_t = T_affine @ X\n",
    "\n",
    "    # If you want to return the total (summed) squared residual use the following\n",
    "    #residual_error = np.sum(np.power((X_dash - X_t)*select_flag, 2))\n",
    "\n",
    "    # If you want residual error per matching point in terms of pixels\n",
    "    residual_error = np.sum(np.power(X_dash - X_t, 2)*select_flag)\n",
    "    residual_error = np.sqrt(residual_error/np.sum(select_flag))\n",
    "\n",
    "    return(residual_error, T_affine)\n",
    "\n",
    "def ransac_N(m=4, u=0.3, p=0.99) :\n",
    "    #m: minimum number of data units needed to estimate\n",
    "    #u: what fraction of data are inliers\n",
    "    #p: how confident we want to be in our estimate\n",
    "\n",
    "    N = np.log(1 - p)/np.log(1 - np.power(u, m))\n",
    "    return(int(N))\n",
    "\n",
    "def detect_intliers (X, X_dash, Transform, acceptable_error = 2) :\n",
    "    # input: two 2D points sets, 3 by N arrays of homogeneous representation of the points\n",
    "    # Transform: estimated transform matrix to use to detect inliers.\n",
    "    # acceptable_error - amount of average pixel error that is acceptable for inliers\n",
    "    # output is an 1D array of size equal to the number of points, N, with 0 or 1,\n",
    "    #        with 1 indicating the corresponding point is an inlier\n",
    "\n",
    "    X_t = Transform @ X\n",
    "    X_t = np.divide(X_t, X_t [2,:]) # normalized homogenous coordinates\n",
    "    X_dash = np.divide(X_dash, X_dash [2,:]) # normalized homogenous coordinates\n",
    "\n",
    "    error =  (X_dash - X_t)\n",
    "    residual_error = np.sum(np.power(error, 2), axis=0)\n",
    "    inliers  = np.where(residual_error < (acceptable_error*acceptable_error), 1, 0)\n",
    "\n",
    "    return(inliers)\n",
    "\n",
    "def ransac_fit (Points_1, Points_2, fit_function, acceptable_error = 2, min_samples=3, fraction_inlier = 0.05) :\n",
    "    # input: two 2D points sets, each of size N by 2\n",
    "    # fit_function: a function that computes the best fit, e.g. affine_fit\n",
    "    # acceptable_error - amount of average pixel error that is acceptable\n",
    "    # min_samples: number of samples per iteration\n",
    "    # fraction_inlier: what fraction of the data are inliers\n",
    "    # outputs: residual error, the best inlier flags, and the best fitting transformation\n",
    "\n",
    "    # Rearrange the N by 2 sized points variable into 3 by N arrays of homogeneous representation of the points\n",
    "    X = np.row_stack((Points_1.transpose(1,0), np.ones((1, Points_1.shape[0]))))\n",
    "    X_dash = np.row_stack((Points_2.transpose(1,0), np.ones((1, Points_2.shape[0]))))\n",
    "\n",
    "    N = X.shape[1]\n",
    "    best_inliers = np.zeros((N,))\n",
    "    N_ransac = ransac_N(m=min_samples, u=fraction_inlier, p=0.99)\n",
    "    print('Repeating RANSAC {} times'.format(N_ransac))\n",
    "    for i in range(N_ransac) :\n",
    "        indices = np.random.choice(range(N), min_samples, replace=False)\n",
    "        selected_pts = np.zeros((N,))\n",
    "        selected_pts[indices] = 1\n",
    "        residual_error, transform = fit_function (X, X_dash, selected_pts)\n",
    "        inliers = detect_intliers (X, X_dash, transform, acceptable_error)\n",
    "        if (np.sum(inliers) > np.sum(best_inliers)) :\n",
    "            best_inliers = inliers\n",
    "            transform_best = transform\n",
    "            print('inlier:', np.sum(best_inliers), 'of ', N)\n",
    "\n",
    "    selected_pts = np.zeros((N,))\n",
    "    selected_pts[np.nonzero(best_inliers)] = 1\n",
    "    residual_error, transform_best = fit_function (X, X_dash, selected_pts)\n",
    "    return(residual_error, best_inliers, transform_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Please complete the content in the TODO block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def fit_Newton_autograd (Input_points, Output_points) :\n",
    "    # Input_points, Output_points -- N by 2 sized arrays of (x, y) coordinates of points.\n",
    "\n",
    "    # ------set up the input and output tensors -------------------------------\n",
    "    # create homogeneous representation of input points (3 by N sized array)\n",
    "    # and turn into torch tensors\n",
    "    X1 = np.row_stack((Input_points.transpose(1,0), np.ones((1, Input_points.shape[0]))))\n",
    "    X = torch.tensor(X1, device = device, requires_grad=False) # homogenous coordinates\n",
    "\n",
    "    # torch tensor for the output points\n",
    "    # (2 by N sized, do not need this to be homogemnous as we will not operate on them)\n",
    "    X_dash = torch.tensor(Output_points.transpose(1,0), device = device, requires_grad=False) # non-homogenous coordinates\n",
    "\n",
    "    h_22 = torch.tensor([0], device = device, requires_grad=False)\n",
    "    # fixed entry of the homography matrix, need to treat it separately\n",
    "    # as we will not be computing derivative with respect to it\n",
    "\n",
    "    # ----------------------Compute initial estimate ----------------------------\n",
    "    print('Performing affine fit with outlier detection to be used as initial parameters')\n",
    "    av_residual, matches_selected, T_affine =  ransac_fit (Input_points, Output_points, fit_affine, acceptable_error=4)\n",
    "    print('Average inlier error:', av_residual, '\\nT_affine=\\n', T_affine)\n",
    "    matches_selected = torch.tensor(np.diag(matches_selected), dtype = float, device = device, requires_grad=False)\n",
    "\n",
    "    # initialize the 8 parameters of the perspective transform matrix. Recall, h_22 = 0, hence only 8\n",
    "    h_ = (T_affine - np.eye(3)).reshape(9,)[0:8]\n",
    "    # we remove an identity matrix from T_affine as per parameterization convention (p=0, represents the identity matrix)\n",
    "    h_8_est = torch.tensor(h_, device = device, requires_grad=True)\n",
    "\n",
    "\n",
    "    #---------------define the loss function to be optimized-----------------------------\n",
    "    def fit_error (h_8): # just 8 free parameters\n",
    "        # Computes the fit error of 2D homography fit between X and X_dash using h_8 parameters\n",
    "        # This is what we want to minimize by varying the h_8.\n",
    "        # Uses variables defined outside of the function: X, X_dash, matches_selected\n",
    "\n",
    "        H = torch.cat((h_8, h_22), 0)\n",
    "        X_t = torch.matmul(H.reshape(3,3) + torch.eye(3, device = device), X)\n",
    "\n",
    "        Xt_nh = torch.div(X_t, X_t [2,:]) # normalized homogenous coordinates\n",
    "        Xt_nh = Xt_nh [0:2,:]\n",
    "\n",
    "        ################################# TODO ####################################\n",
    "        # Hint: Calculate the loss based on the difference between X_dash and Xt_nh.\n",
    "\n",
    "        ###########################################################################\n",
    "        return(torch.pow(Xloss, 2).sum())\n",
    "\n",
    "    #-----------------------initialize the learning parameters----------------------------------------\n",
    "    learning_rate = 1\n",
    "    prev_residual = 99999.0\n",
    "    exit_flag = False\n",
    "    max_iterations = 1000\n",
    "    t = 0\n",
    "    #------------------------------------estimation iterations----------------------------------------\n",
    "    while (exit_flag == False):\n",
    "        ############################### TODO ##################################\n",
    "        # Hint: Use torch.autograd.functional.hessian to compute the Hessian of the loss function.\n",
    "\n",
    "        #######################################################################\n",
    "        \n",
    "        residual = fit_error (h_8_est)/X.shape[1]\n",
    "        residual.backward()\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ######################## TODO #################################\n",
    "            # Hint: Use gradient descent with the Hessian to update h_8_est.\n",
    "\n",
    "            ###############################################################\n",
    "            if prev_residual > residual.item() :\n",
    "                prev_residual = residual.item() # *.item() gets the scalar value held.\n",
    "            else :\n",
    "                exit_flag = True\n",
    "            if (np.remainder(t, 100) == 0) :\n",
    "                print(t, np.sqrt(residual.item())) \n",
    "\n",
    "            # Zero-out the gradients after updating weights\n",
    "            h_8_est.grad = None\n",
    "        t = t+1\n",
    "        if (t > max_iterations) :\n",
    "            exit_flag = True\n",
    "\n",
    "    H = torch.cat((h_8_est, h_22), 0).reshape(3, 3) + torch.eye(3, device = device)\n",
    "\n",
    "    return(torch.sqrt(residual), H.detach().cpu().numpy(), matches_selected.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_1 = io.imread(\"../data/ransac_img1.png\")\n",
    "image_2 = io.imread(\"../data/ransac_img2.png\")\n",
    "\n",
    "image_1 = cv2.cvtColor(image_1, cv2.COLOR_RGB2BGR) # change from scikit RGB to cv2 BGR format for color images\n",
    "image_2 = cv2.cvtColor(image_2, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "image_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY) # change from color to gray\n",
    "image_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "X_1, X_2, keypoints_1, keypoints_2, matches = detect_and_match_keypoints (image_1, image_2)\n",
    "top_matches = 100\n",
    "X_1 = X_1[:top_matches,:]\n",
    "X_2 = X_2[:top_matches,:]\n",
    "\n",
    "######################## TODO #################################\n",
    "#-----------call to non linear fitting function-------------------\n",
    "\n",
    "###############################################################\n",
    "\n",
    "T_inv = np.linalg.inv(T)\n",
    "print('Residual error={}\\n T =\\n{}\\n T_inv =\\n{}'.format(residual, T, T_inv))\n",
    "\n",
    "warped_1_into_2 = warp(image_1, inverse_map=T_inv, output_shape=image_2.shape)\n",
    "warped_2_into_1 = warp(image_2, inverse_map=T, output_shape=image_1.shape)\n",
    "\n",
    "\n",
    "#----------------------------------Display-------------------------------------------------------\n",
    "# draw matches\n",
    "matches_selected = (np.where(matches_selected==1)[0])\n",
    "matches_selected = np.take(matches, matches_selected)\n",
    "match_image = cv2.drawMatches(image_1, keypoints_1, image_2, keypoints_2, matches_selected, image_2, flags=2)\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "plt.subplot(221)\n",
    "plt.imshow(image_1, 'gray')\n",
    "plt.plot(X_1[:,0], X_1[:,1], 'bo')\n",
    "plt.subplot(222)\n",
    "plt.imshow(image_2, 'gray')\n",
    "plt.plot(X_2[:,0], X_2[:,1], 'ro')\n",
    "plt.subplot(212)\n",
    "plt.imshow(match_image)\n",
    "\n",
    "# build an RGB image with the registered images -- should be white if match is perfect\n",
    "seq_im_1_into_2 = np.zeros((image_2.shape[0], image_2.shape[1], 3))\n",
    "seq_im_1_into_2[..., 0] = warped_1_into_2\n",
    "seq_im_1_into_2[..., 1] = image_2/255\n",
    "seq_im_1_into_2[..., 2] = image_2/255\n",
    "\n",
    "seq_im_2_into_1 = np.zeros((image_1.shape[0], image_1.shape[1], 3))\n",
    "seq_im_2_into_1[..., 0] = warped_2_into_1\n",
    "seq_im_2_into_1[..., 1] = image_1/255\n",
    "seq_im_2_into_1[..., 2] = image_1/255\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(image_1, 'gray')\n",
    "plt.title('Image 1 (from image)')\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(warped_2_into_1, 'gray')\n",
    "plt.title('Image 2 mapped to register with Image 1')\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(seq_im_2_into_1, 'gray')\n",
    "plt.title('Color image create with Image 1 and remapped Image 2')\n",
    "\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(image_2, 'gray')\n",
    "plt.title('Image 2 (to image)')\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.imshow(warped_1_into_2, 'gray')\n",
    "plt.title('Image 1 mapped to register with Image 2')\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.imshow(seq_im_1_into_2, 'gray')\n",
    "plt.title('Color image create with Image 2 and remapped Image 1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
